{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AkHQ: http://localhost:8082  \n",
    "Spark-Master http://localhost:8083  \n",
    "Spark-Worker-1 http://localhost:8084\n",
    "\n",
    "https://github.com/aehrc/pathling/tree/issue/452/lib/python#python-api-for-pathling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE PARTITION NUMBER IN save FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "appName = \"Kafka, Spark and FHIR Data\"\n",
    "master = \"local[8]\" # test for writing checkpoint to local filesystem - SET a NUMBER IN THE BRACKETS FOR PARTITIONS 3-4 TIMES THE NUMBER OF CPU CORES IN YOUR CLUSTER\n",
    "#master = \"spark://spark-master:7077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pathling.etc import find_jar\n",
    "\n",
    "\n",
    "#.config(\"spark.worker.cores\", \"10\") \\ #.config(\"spark.worker.cores\", \"8\") \\\n",
    "    #.config(\"spark.worker.memory\", \"30g\") \\ #.config(\"spark.worker.memory\", \"24\") \\\n",
    "    #.config(\"spark.executor.memory\", \"26g\") \\ #.config(\"spark.executor.memory\", \"20g\") \\\n",
    "    #.config(\"spark.driver.memory\", \"28g\") \\ #.config(\"spark.driver.memory\", \"22g\") \\\n",
    "    \n",
    "    #spark.executer und spark.driver.memory reduzieren\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"1000\") \\\n",
    "    .config(\"spark.worker.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.network.timeout\", \"60000s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.addFile(find_jar())\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoints\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, when\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bring Pathling into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathling\n",
    "from pathling import PathlingContext, Expression as exp\n",
    "from pyspark.sql.functions import regexp_replace, col, explode, concat_ws, explode_outer, first, to_date, max\n",
    "\n",
    "ptl = PathlingContext.create(spark = spark, enable_extensions = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Parquet-Ordner muss immer wieder gelöscht werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kafka:\n",
    "    print(\"Read data from kafka\")\n",
    "\n",
    "    kafka_server = \"svm-ap-dizk8s1q.srv.uk-erlangen.de:32386\" # node forwarding\n",
    "\n",
    "    df = spark \\\n",
    "      .read \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "      .option(\"assign\", \"\"\"{\"fhir.pacs.imagingStudy\":[0,1]}\"\"\") \\\n",
    "      .load()\n",
    "\n",
    "    #          .writeStream \\ -- remove stream AND           .trigger(once=True) \\\n",
    "\n",
    "    kafka_data = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "    kafka_data.show()\n",
    "\n",
    "else:\n",
    "    print(\"Read data from folder\")\n",
    "\n",
    "    bundles_dir = 'testdata'\n",
    "    bundles = ptl.spark.read.text(bundles_dir, wholetext=True)\n",
    "\n",
    "    # ImagingStudies\n",
    "    imaging_studies = ptl.encode_bundle(bundles, \"ImagingStudy\")\n",
    "    imaging_studies_dataset = ptl.read.datasets({\"ImagingStudy\": imaging_studies})\n",
    "    imaging_studies_dataset.write.parquet(\"./parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGING_STUDIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathling.datasource.DataSource"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaging_data_kafka = ptl.encode_bundle(kafka_data.select(\"value\"), 'ImagingStudy')\n",
    "imaging_data_kafka_dataset = ptl.read.datasets({\"ImagingStudy\": imaging_data_kafka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imaging_studies = imaging_data_kafka_dataset.extract(\"ImagingStudy\", columns=[\n",
    "    exp(\"numberOfSeries\", \"no_series\"),\n",
    "    exp(\"numberOfInstances\", \"no_instances\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "studies_no = imaging_studies.count()\n",
    "\n",
    "imaging_series = imaging_data_kafka_dataset.extract(\"ImagingStudy\", columns=[\n",
    "    exp(\"series.bodySite.code\", \"bodysite\"),\n",
    "    exp(\"series.modality.code\", \"modality\"),\n",
    "    exp(\"series.laterality.code\", \"laterality\"), \n",
    "    exp(\"series.numberOfInstances\", \"s_no_instances\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "series_no = imaging_series.count()\n",
    "instance_no = imaging_series.agg({\"s_no_instances\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "result_bodysite = imaging_series.groupBy(\"bodysite\").count().orderBy(\"count\", ascending=False)\n",
    "result_bodysite_with_modality = imaging_series.filter(col(\"modality\") != \"SR\").filter(col(\"modality\") != \"DOC\").filter(col(\"modality\") != \"SEG\").filter(col(\"modality\") != \"PR\").filter(col(\"modality\") != \"KO\").filter(col(\"modality\") != \"OT\").filter(col(\"modality\") != \"REG\").filter(col(\"modality\") != \"SC\").filter(col(\"modality\") != \"RTSTRUCT\").groupBy(\"bodysite\").count().orderBy(\"count\", ascending=False)\n",
    "result_laterality = imaging_series.groupBy(\"laterality\").count()\n",
    "result_laterality_L_bodySite = imaging_series.filter(col(\"laterality\") == \"L\").groupBy(\"bodySite\").count().orderBy(\"count\", ascending=False)\n",
    "result_laterality_R_bodySite = imaging_series.filter(col(\"laterality\") == \"R\").groupBy(\"bodySite\").count().orderBy(\"count\", ascending=False)\n",
    "result_modality = imaging_series.groupBy(\"modality\").count().orderBy(\"count\", ascending=False)\n",
    "result_instances_modality = imaging_series.groupBy(\"modality\").agg({\"s_no_instances\": \"avg\"}).orderBy(\"avg(s_no_instances)\", ascending=False)\n",
    "result_instances_bodysite = imaging_series.filter(col(\"bodysite\") == \"10200004\").agg({\"s_no_instances\": \"avg\"})\n",
    "result_modality_bodysite = imaging_series.filter(col(\"modality\") == \"XA\").groupBy(\"bodysite\").count().orderBy(\"count\", ascending=False)\n",
    "result_series = imaging_studies.agg({\"no_series\": \"avg\"})\n",
    "result_instances = imaging_studies.agg({\"no_instances\": \"avg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def save_final_df(final_df, df_name):\n",
    "    output_folder = \"csv-output\"\n",
    "    output_file = str(df_name + \"_partition_0-1.csv\")  # RENAME numbers HERE!\n",
    "    \n",
    "    final_df_pandas = final_df.toPandas()\n",
    "\n",
    "    output_path_filename = os.path.join(\n",
    "        output_folder, output_file\n",
    "    )\n",
    "    print(output_path_filename)\n",
    "    print(\"###### current dir: \", os.getcwd())\n",
    "    print(\"###### output_path_filename : \", output_path_filename)\n",
    "\n",
    "    final_df_pandas.to_csv(output_path_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Total number of studies: \"+str(studies_no))\n",
    "print(\"Total number of series: \"+str(series_no))\n",
    "print (\"Total number of instances: \"+str(instance_no))\n",
    "# #result_bodysite.show()\n",
    "save_final_df(result_bodysite, \"result_bodysite\")\n",
    "# print(\"Bodysite without non-acquisition modalities\")\n",
    "# result_bodysite_with_modality.show()\n",
    "save_final_df(result_bodysite_with_modality, \"result_bodysite_with_modality\")\n",
    "# #result_modality.show()\n",
    "save_final_df(result_modality, \"result_modality\")\n",
    "# #result_laterality.show()\n",
    "save_final_df(result_laterality, \"result_laterality\")\n",
    "# print(\"Laterality left\")\n",
    "# result_laterality_L_bodySite.show()\n",
    "save_final_df(result_laterality_L_bodySite, \"result_laterality_L_bodySite\")\n",
    "# print(\"Laterality right\")\n",
    "# result_laterality_R_bodySite.show()\n",
    "save_final_df(result_laterality_R_bodySite, \"result_laterality_R_bodySite\")\n",
    "# #result_series.show()\n",
    "save_final_df(result_series, \"result_series\")\n",
    "# result_instances.show()\n",
    "save_final_df(result_instances, \"result_instances\")\n",
    "# result_instances_modality.show()\n",
    "save_final_df(result_instances_modality, \"result_instances_modality\")\n",
    "# result_instances_bodysite.show()\n",
    "save_final_df(result_instances_bodysite, \"result_instances_bodysite\")\n",
    "# result_modality_bodysite.show()\n",
    "save_final_df(result_modality_bodysite, \"result_modality_bodysite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ACHTUNG - zwar übersichtlicher, aber kann sehr langsam sein\n",
    "imaging_studies.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parquet Ordner löschen\n",
    "! rm -rf parquet"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
